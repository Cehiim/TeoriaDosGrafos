{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cehiim/TeoriaDosGrafos/blob/main/Projeto/grafos_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX9e_J1rfR8k"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4Fd36vBfY_m"
      },
      "source": [
        "## Integração dos pacotes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YI_J43xnrFIo"
      },
      "source": [
        "O pacote `vectordb2` é usado para armazenar e recuperar textos usando técnicas de *chunking* (segmentação de texto), *embedding* (conversão de texto para vetores numéricos) e busca vetorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "yy3H5b34UlKV",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "73141ded-5624-40e6-fb62-909f609c22fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: vectordb2 in /usr/local/lib/python3.10/dist-packages (0.1.9)\n",
            "Requirement already satisfied: torch>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from vectordb2) (2.4.1+cu121)\n",
            "Requirement already satisfied: transformers>=4.10.0 in /usr/local/lib/python3.10/dist-packages (from vectordb2) (4.44.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from vectordb2) (1.26.4)\n",
            "Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from vectordb2) (1.3.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from vectordb2) (1.13.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (from vectordb2) (3.1.1)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (from vectordb2) (1.8.0.post1)\n",
            "Requirement already satisfied: tensorflow-text in /usr/local/lib/python3.10/dist-packages (from vectordb2) (2.17.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->vectordb2) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24.0->vectordb2) (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->vectordb2) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->vectordb2) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->vectordb2) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->vectordb2) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->vectordb2) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.9.0->vectordb2) (2024.6.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->vectordb2) (0.24.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->vectordb2) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->vectordb2) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->vectordb2) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->vectordb2) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->vectordb2) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->vectordb2) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.10.0->vectordb2) (4.66.5)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->vectordb2) (10.4.0)\n",
            "Requirement already satisfied: tensorflow<2.18,>=2.17.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-text->vectordb2) (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->vectordb2) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->vectordb2) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->vectordb2) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->vectordb2) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->vectordb2) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->vectordb2) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->vectordb2) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->vectordb2) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->vectordb2) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->vectordb2) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->vectordb2) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->vectordb2) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->vectordb2) (2.4.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->vectordb2) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->vectordb2) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->vectordb2) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->vectordb2) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow<2.18,>=2.17.0->tensorflow-text->vectordb2) (0.37.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.10.0->vectordb2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.10.0->vectordb2) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.10.0->vectordb2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.10.0->vectordb2) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.9.0->vectordb2) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.9.0->vectordb2) (1.3.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow<2.18,>=2.17.0->tensorflow-text->vectordb2) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17.0->tensorflow-text->vectordb2) (13.8.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17.0->tensorflow-text->vectordb2) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow<2.18,>=2.17.0->tensorflow-text->vectordb2) (0.12.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17.0->tensorflow-text->vectordb2) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17.0->tensorflow-text->vectordb2) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow<2.18,>=2.17.0->tensorflow-text->vectordb2) (3.0.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17.0->tensorflow-text->vectordb2) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow<2.18,>=2.17.0->tensorflow-text->vectordb2) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow<2.18,>=2.17.0->tensorflow-text->vectordb2) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "%pip install vectordb2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZDIGMyXB04o"
      },
      "source": [
        "O pacote requests pode ser usado para recuperar o arquivo por meio de requisição em HTTP (esse pacote é opcional)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "FucP-UzdB1B3",
        "outputId": "f55193a6-e5c7-4336-c136-d768c6b7cb05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "%pip install requests"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1OtxcQzT2JAX"
      },
      "source": [
        "O pacote `networkx` é usado para a criação, manipulação e representação de grafos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "hVwSlpPSeJPm",
        "outputId": "15e3a316-8dfe-4fcb-ec17-12d543c64cc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (3.3)\n"
          ]
        }
      ],
      "source": [
        "%pip install networkx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjtCEtpOCxs2"
      },
      "source": [
        "Importação das bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "DA_HyrBKC6J0"
      },
      "outputs": [],
      "source": [
        "from vectordb import Memory\n",
        "import requests\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt # Será usado para apresentação visual do grafo\n",
        "import os # Será usado métodos para limpar o terminal para atualizar a interface em cada iteração do sistema\n",
        "import time # Será usado método de espera para atualizar a interface gradualmente"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUL4VcKo2JAl",
        "jp-MarkdownHeadingCollapsed": true
      },
      "source": [
        "## Classe Grafo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "yAaYISkbfzOE"
      },
      "outputs": [],
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Mon Feb 13 13:59:10 2023\n",
        "\n",
        "@author: icalc\n",
        "\"\"\"\n",
        "class Grafo:\n",
        "    TAM_MAX_DEFAULT = 100 # qtde de vértices máxima default\n",
        "    # construtor da classe grafo\n",
        "    def __init__(self, n=TAM_MAX_DEFAULT):\n",
        "        self.n = n # número de vértices\n",
        "        self.m = 0 # número de arestas\n",
        "        # matriz de adjacência\n",
        "        self.adj = [[0 for i in range(n)] for j in range(n)]\n",
        "\n",
        "\t# Insere uma aresta no Grafo tal que\n",
        "\t# v é adjacente a w\n",
        "    def insereA(self, v, w):\n",
        "        if self.adj[v][w] == 0:\n",
        "            self.adj[v][w] = 1\n",
        "            self.m+=1 # atualiza qtd arestas\n",
        "\n",
        "# remove uma aresta v->w do Grafo\n",
        "    def removeA(self, v, w):\n",
        "        if(v == w):\n",
        "            return\n",
        "        # testa se temos a aresta\n",
        "        if self.adj[v][w] == 1:\n",
        "            self.adj[v][w] = 0\n",
        "            self.m -= 1  # atualiza qtd arestas\n",
        "\n",
        "\t# Apresenta o Grafo contendo\n",
        "\t# número de vértices, arestas\n",
        "\t# e a matriz de adjacência obtida\n",
        "    def show(self):\n",
        "        print(f\"\\n n: {self.n:2d} \", end=\"\")\n",
        "        print(f\"m: {self.m:2d}\\n\")\n",
        "        for i in range(self.n):\n",
        "            for w in range(self.n):\n",
        "                if self.adj[i][w] == 1:\n",
        "                    print(f\"Adj[{i:2d},{w:2d}] = 1 \", end=\"\")\n",
        "                else:\n",
        "                    print(f\"Adj[{i:2d},{w:2d}] = 0 \", end=\"\")\n",
        "            print(\"\\n\")\n",
        "        print(\"\\nfim da impressao do grafo.\" )\n",
        "\n",
        "\n",
        "\t# Apresenta o Grafo contendo\n",
        "\t# número de vértices, arestas\n",
        "\t# e a matriz de adjacência obtida\n",
        "    # Apresentando apenas os valores 0 ou 1\n",
        "    def showMin(self):\n",
        "        print(f\"\\n n: {self.n:2d} \", end=\"\")\n",
        "        print(f\"m: {self.m:2d}\\n\")\n",
        "        for i in range(self.n):\n",
        "            for w in range(self.n):\n",
        "                if self.adj[i][w] == 1:\n",
        "                    print(\" 1 \", end=\"\")\n",
        "                else:\n",
        "                    print(\" 0 \", end=\"\")\n",
        "            print(\"\\n\")\n",
        "        print(\"\\nfim da impressao do grafo.\" )\n",
        "\n",
        "    def plota_grafo(self):\n",
        "        # Criar um grafo dirigido usando a matriz de adjacência\n",
        "        G = nx.DiGraph()  # Grafo dirigido\n",
        "\n",
        "        # Adicionar vértices e arestas\n",
        "        for i in range(self.n):\n",
        "            for j in range(self.n):\n",
        "                if self.adj[i][j] == 1:\n",
        "                    G.add_edge(i, j)\n",
        "\n",
        "        # Plotar o grafo\n",
        "        plt.figure(figsize=(8, 8))\n",
        "        pos = nx.spring_layout(G)  # Layout para a posição dos nós\n",
        "        nx.draw(G, pos, with_labels=True, node_color='lightblue', node_size=500, font_size=10, font_color='black', arrowstyle='-|>', arrowsize=20)\n",
        "        plt.title(f\"Grafo com {self.n} vértices e {self.m} arestas\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2sI3d7Qz7ck"
      },
      "source": [
        "## Classe GrafoND (Grafo não-direcionado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "F1sOCb5P0ITt"
      },
      "outputs": [],
      "source": [
        "# Grafo como uma matriz de adjacência não-direcionado\n",
        "class GrafoND(Grafo): # Ex 7\n",
        "    def __init__(self, n):\n",
        "        super().__init__(n)\n",
        "\n",
        "    def insereA(self, v, w):\n",
        "        if(v == w):\n",
        "            return\n",
        "        if self.adj[v][w] == 0:\n",
        "            self.adj[v][w] = 1\n",
        "            self.adj[w][v] = 1\n",
        "            self.m += 1  # atualiza qtd arestas\n",
        "\n",
        "# remove uma aresta v->w do Grafo\n",
        "    def removeA(self, v, w):\n",
        "        if(v == w):\n",
        "            return\n",
        "        # testa se temos a aresta\n",
        "        if self.adj[v][w] == 1:\n",
        "            self.adj[v][w] = 0\n",
        "            self.adj[w][v] = 0\n",
        "            self.m -= 1  # atualiza qtd arestas\n",
        "\n",
        "    def show(self):\n",
        "        print(f\"\\n n: {self.n:2d} \", end=\"\")\n",
        "        print(f\"m: {self.m:2d}\\n\")\n",
        "        for i in range(int(self.n)):\n",
        "            for w in range(self.n):\n",
        "                print(f\"Adj[{i:2d},{w:2d}] = {self.adj[i][w]:.2f} \", end=\"\")\n",
        "            print(\"\\n\")\n",
        "        print(\"\\nfim da impressao do grafo.\" )\n",
        "\n",
        "    #Verifica se o grafo é completo Ex 10)\n",
        "    def ehCompleto(self):\n",
        "        if((self.n ** 2 - self.n)/ 2 == self.m):\n",
        "            return \"O grafo é completo\"\n",
        "        else:\n",
        "            return \"O grafo não é completo\"\n",
        "\n",
        "    def conexidade(self): # Ex 13\n",
        "        if(self.n > 1 and self.m == 0):\n",
        "            return \"O grafo não é conexo\"\n",
        "        passou = [0]\n",
        "        for i in range(self.n):\n",
        "            if(not i in passou):\n",
        "                return \"O grafo não é conexo\"\n",
        "            for j in range(self.n):\n",
        "                if(i != j and self.adj[i][j] != 0 and not j in passou):\n",
        "                    passou.append(j)\n",
        "                    #print(passou)\n",
        "        return \"O grafo é conexo\"\n",
        "\n",
        "    def removeV(self, vertice): # Ex 24\n",
        "        if(vertice >= self.n):\n",
        "            return False\n",
        "        for i in range(self.n-1):\n",
        "            if(i >= vertice): # Substitui as conexões do vértice a ser retirado e\n",
        "                              # os vértices posteriores a ele com as conexões do próximo vértice\n",
        "                self.adj[i] = self.adj[i+1]\n",
        "            self.removeA(i,vertice)\n",
        "            self.adj[i].pop(vertice) # Remove o vértice escolhido da linha da matriz\n",
        "        self.adj.pop() # Remove a última linha da matriz\n",
        "        self.n -= 1\n",
        "        return True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nt727Vzj0Kos"
      },
      "source": [
        "## Classe GrafoNDR (Grafo não-direcionado rotulado)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "lKyCUwc80StM"
      },
      "outputs": [],
      "source": [
        "# Grafo como uma matriz de adjacência não-direcionado rotulado\n",
        "class GrafoNDR(GrafoND): # Ex 8\n",
        "# Não bota o init, vai bugar a classe\n",
        "\n",
        "    def insereA(self, v, w, p):\n",
        "        if(v == w):\n",
        "            return\n",
        "        if self.adj[v][w] == 0:\n",
        "            self.adj[v][w] = p\n",
        "            self.adj[w][v] = p\n",
        "            self.m += 1  # atualiza qtd arestas\n",
        "\n",
        "    def show(self):\n",
        "        print(f\"\\n n: {self.n:2d} \", end=\"\")\n",
        "        print(f\"m: {self.m:2d}\\n\")\n",
        "        for i in range(self.n):\n",
        "            for w in range(self.n):\n",
        "                print(f\"Adj[{i:2d},{w:2d}] = {self.adj[i][w]:.2f} \", end=\"\")\n",
        "            print(\"\\n\")\n",
        "        print(\"\\nfim da impressao do grafo.\" )\n",
        "\n",
        "\n",
        "\t# Apresenta o Grafo contendo\n",
        "\t# número de vértices, arestas\n",
        "\t# e a matriz de adjacência obtida\n",
        "    # Apresentando apenas os valores 0 ou 1\n",
        "    def showMin(self):\n",
        "        print(f\"\\n n: {self.n:2d} \", end=\"\")\n",
        "        print(f\"m: {self.m:2d}\\n\")\n",
        "        for i in range(self.n):\n",
        "            for w in range(self.n):\n",
        "                print(f\" {self.adj[i][w]:.2f} \", end=\"\")\n",
        "            print(\"\\n\")\n",
        "        print(\"\\nfim da impressao do grafo.\" )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classe"
      ],
      "metadata": {
        "id": "-Uoi9mH_L1yi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMuxrf07kEw_"
      },
      "source": [
        "## Classe Memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6uWsA4dkZwE"
      },
      "source": [
        "Aqui é utilizado a biblioteca VectorDB para criar uma memória virtual.\n",
        "\n",
        "```\n",
        "memoria = Memory(chunking_strategy={\"mode\": \"sliding_window\", \"window_size\": 1, \"overlap\": 0})\n",
        "```\n",
        "\n",
        "- `chunking_strategy`: Define a estratégia de fragmentação dos dados. No modo \"sliding_window\", os dados são divididos em *chunks* (pedaços de texto) de tamanho fixo.\n",
        "\n",
        "- `window_size`: Define a quantidade de palavras que um *chunk* representa. Neste caso, cada *chunk* representa uma palavra.\n",
        "\n",
        "- `overlap`: Define quantos elementos de sobreposição existirão entre os *chunks* adjacentes. Neste caso, não haverá sobreposição já que as palavras usadas não formam frases, logo são independentes uma das outras."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### INIT"
      ],
      "metadata": {
        "id": "xvaddfZSJlrp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "    def __init__(\n",
        "        self,\n",
        "        memory_file: str = None,\n",
        "        chunking_strategy: dict = None,\n",
        "        embeddings: Union[BaseEmbedder, str] = \"normal\",\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initializes the Memory class.\n",
        "\n",
        "        :param memory_file: a string containing the path to the memory file. (default: None)\n",
        "        :param chunking_strategy: a dictionary containing the chunking mode (default: {\"mode\": \"sliding_window\"}).\n",
        "        :param embedding_model: a string containing the name of the pre-trained model to be used for embeddings (default: \"sentence-transformers/all-MiniLM-L6-v2\").\n",
        "        \"\"\"\n",
        "        self.memory_file = memory_file\n",
        "\n",
        "        if memory_file is None:\n",
        "            self.memory = []\n",
        "            self.metadata_memory = []\n",
        "        else:\n",
        "            load = Storage(memory_file).load_from_disk()       \n",
        "            self.memory = [] if len(load) != 1 else load[0][\"memory\"]\n",
        "            self.metadata_memory = [] if len(load) != 1 else load[0][\"metadata\"]\n",
        "\n",
        "        if chunking_strategy is None:\n",
        "            chunking_strategy = {\"mode\": \"sliding_window\"}\n",
        "        self.chunker = Chunker(chunking_strategy)\n",
        "\n",
        "        self.metadata_index_counter = 0\n",
        "        self.text_index_counter = 0\n",
        "\n",
        "        if isinstance(embeddings, str):\n",
        "            self.embedder = Embedder(embeddings)\n",
        "        elif isinstance(embeddings, BaseEmbedder):\n",
        "            self.embedder = embeddings\n",
        "        else:\n",
        "            raise TypeError(\"Embeddings must be an Embedder instance or string\")\n",
        "\n",
        "        self.vector_search = VectorSearch()\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "HGyXFhYFJHHE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SAVE"
      ],
      "metadata": {
        "id": "qaQx_0LIJ6Fw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "    def save(\n",
        "        self,\n",
        "        texts,\n",
        "        metadata: Union[List, List[dict], None] = None,\n",
        "        memory_file: str = None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Saves the given texts and metadata to memory.\n",
        "\n",
        "        :param texts: a string or a list of strings containing the texts to be saved.\n",
        "        :param metadata: a dictionary or a list of dictionaries containing the metadata associated with the texts.\n",
        "        :param memory_file: a string containing the path to the memory file. (default: None)\n",
        "        \"\"\"\n",
        "\n",
        "        if not isinstance(texts, list):\n",
        "            texts = [texts]\n",
        "\n",
        "        if metadata is None:\n",
        "            metadata = []\n",
        "        elif not isinstance(metadata, list):\n",
        "            metadata = [metadata]\n",
        "\n",
        "        # Extend metadata to be the same length as texts, if it's shorter.\n",
        "        metadata += [{}] * (len(texts) - len(metadata))\n",
        "\n",
        "        for meta in metadata:\n",
        "            self.metadata_memory.append(meta)\n",
        "\n",
        "        meta_index_start = (\n",
        "            self.metadata_index_counter\n",
        "        )  # Starting index for this save operation\n",
        "        self.metadata_index_counter += len(\n",
        "            metadata\n",
        "        )  # Update the counter for future save operations\n",
        "\n",
        "        if memory_file is None:\n",
        "            memory_file = self.memory_file\n",
        "\n",
        "        text_chunks = [self.chunker(text) for text in texts]\n",
        "        chunks_size = [len(chunks) for chunks in text_chunks]\n",
        "\n",
        "        flatten_chunks = list(itertools.chain.from_iterable(text_chunks))\n",
        "\n",
        "        embeddings = self.embedder.embed_text(flatten_chunks)\n",
        "\n",
        "        text_index_start = (\n",
        "            self.text_index_counter\n",
        "        )  # Starting index for this save operation\n",
        "        self.text_index_counter += len(texts)\n",
        "\n",
        "        # accumulated size is end_index of each chunk\n",
        "        for size, end_index, chunks, meta_index, text_index in zip(\n",
        "            chunks_size,\n",
        "            itertools.accumulate(chunks_size),\n",
        "            text_chunks,\n",
        "            range(meta_index_start, self.metadata_index_counter),\n",
        "            range(text_index_start, self.text_index_counter),\n",
        "        ):\n",
        "            start_index = end_index - size\n",
        "            chunks_embedding = embeddings[start_index:end_index]\n",
        "\n",
        "            for chunk, embedding in zip(chunks, chunks_embedding):\n",
        "                entry = {\n",
        "                    \"chunk\": chunk,\n",
        "                    \"embedding\": embedding,\n",
        "                    \"metadata_index\": meta_index,\n",
        "                    \"text_index\": text_index,\n",
        "                }\n",
        "                self.memory.append(entry)\n",
        "\n",
        "        if memory_file is not None:\n",
        "            Storage(self.memory_file).save_to_disk([{\"memory\": self.memory, \"metadata\" :self.metadata_memory}])\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "czHNUzrrKVMA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SEARCH"
      ],
      "metadata": {
        "id": "i5xPO7sqKCUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "    def search(\n",
        "        self, query: str, top_n: int = 5, unique: bool = False, batch_results: str = \"flatten\"\n",
        "    ) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Searches for the most similar chunks to the given query in memory.\n",
        "\n",
        "        :param query: a string containing the query text.\n",
        "        :param top_n: the number of most similar chunks to return. (default: 5)\n",
        "        :param unique: chunks are filtered out to unique texts (default: False)\n",
        "        :param batch_results: if input is list of queries, results can use \"flatten\" or \"diverse\" algorithm\n",
        "        :return: a list of dictionaries containing the top_n most similar chunks and their associated metadata.\n",
        "        \"\"\"\n",
        "\n",
        "        if isinstance(query, list):\n",
        "            query_embedding = self.embedder.embed_text(query)\n",
        "        else:\n",
        "            query_embedding = self.embedder.embed_text([query])[0]\n",
        "\n",
        "        \n",
        "        embeddings = [entry[\"embedding\"] for entry in self.memory]\n",
        "\n",
        "        indices = self.vector_search.search_vectors(query_embedding, embeddings, top_n, batch_results)\n",
        "        if unique:\n",
        "            unique_indices = []\n",
        "            seen_text_indices = set()  # Change the variable name\n",
        "            for i in indices:\n",
        "                text_index = self.memory[i[0]][\n",
        "                    \"text_index\"\n",
        "                ]  # Use text_index instead of metadata_index\n",
        "                if (\n",
        "                    text_index not in seen_text_indices\n",
        "                ):  # Use seen_text_indices instead of seen_meta_indices\n",
        "                    unique_indices.append(i)\n",
        "                    seen_text_indices.add(\n",
        "                        text_index\n",
        "                    )  # Use seen_text_indices instead of seen_meta_indices\n",
        "            indices = unique_indices\n",
        "\n",
        "        results = [\n",
        "            {\n",
        "                \"chunk\": self.memory[i[0]][\"chunk\"],\n",
        "                \"metadata\": self.metadata_memory[self.memory[i[0]][\"metadata_index\"]],\n",
        "                \"distance\": i[1],\n",
        "            }\n",
        "            for i in indices\n",
        "        ]\n",
        "\n",
        "        return results\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "GdwM9rAdKjam"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro4oq-cagY8g"
      },
      "source": [
        "# Métodos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fj8LIYV32JAo"
      },
      "source": [
        "## 1. Ler dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxpnmvFW3RZ_"
      },
      "source": [
        "### Aquisição dos dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPQ65cuK2JAo"
      },
      "source": [
        "Os dados do documento são importados e guardados na variável `dados`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "-7q8lJ7B2JAq"
      },
      "outputs": [],
      "source": [
        "def leArquivoHTTP():\n",
        "  arquivo = requests.get('https://raw.githubusercontent.com/Cehiim/TeoriaDosGrafos/refs/heads/main/Projeto/palavras.txt').text\n",
        "\n",
        "  palavras = arquivo.split()\n",
        "  n_palavras = int(palavras.pop(0))\n",
        "  dados = {\n",
        "      \"n_palavras\": n_palavras,\n",
        "      \"palavras\": palavras\n",
        "  }\n",
        "\n",
        "  return dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "collapsed": true,
        "id": "qbPmOab62JAr"
      },
      "outputs": [],
      "source": [
        "def leArquivo(origem):\n",
        "  with open(origem, 'r', encoding='utf-8') as arquivo:\n",
        "    n_palavras = int(arquivo.readline())\n",
        "\n",
        "    palavras = [\"\"] * n_palavras\n",
        "    for i in range(n_palavras):\n",
        "      palavras[i] = arquivo.readline().strip()\n",
        "\n",
        "  dados = {\n",
        "      \"n_palavras\": n_palavras,\n",
        "      \"palavras\": palavras\n",
        "  }\n",
        "\n",
        "  return dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WJkcynqo2JAr",
        "outputId": "e6c48e27-e684-4572-e1fb-861cf133cb77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'n_palavras': 50, 'palavras': ['Ecossistema', 'Sustentabilidade', 'Biodiversidade', 'Reciclagem', 'Conservação', 'Poluição', 'Desmatamento', 'Reflorestamento', 'Erosão', 'Compostagem', 'Biodegradável', 'Emissões', 'Pegada', 'Recursos', 'Preservação', 'Ecologia', 'Habitat', 'Fauna', 'Flora', 'Agroecologia', 'Bioma', 'Ciclo', 'Desenvolvimento', 'Economia', 'Efluentes', 'Gestão', 'Impacto', 'Mata', 'Amazônia', 'Cerrado', 'Pantanal', 'Biotecnologia', 'Agrofloresta', 'Agricultura', 'Aquicultura', 'Biocombustível', 'Solar', 'Eólica', 'Hidrelétrica', 'Resíduos', 'Saneamento', 'Tratamento', 'Uso', 'Zona', 'Proteção', 'Ambiental', 'Clima', 'Solo', 'Água', 'Floresta']}\n"
          ]
        }
      ],
      "source": [
        "d = leArquivoHTTP()\n",
        "#d = leArquivo(\"./Projeto/palavras.txt\")\n",
        "print(d)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6KpszTx9wbx"
      },
      "source": [
        "### Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kRX7-7r-BjR"
      },
      "source": [
        "Cada palavra é convertida para um vetor numérico e guardada na memória."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "collapsed": true,
        "id": "4PR45aSh97GE"
      },
      "outputs": [],
      "source": [
        "def embedding(memoria, palavras, n_palavras):\n",
        "  for i in range(n_palavras):\n",
        "    memoria.save(palavras[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XCD1aKeV-eSW"
      },
      "source": [
        "### Busca vetorial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RQUG4Hf4-oNM"
      },
      "source": [
        "Quanto menor é a distância, maior é a proximidade semântica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "iFWuMWvQpgFW"
      },
      "outputs": [],
      "source": [
        "def buscaVetorial(memoria, palavra):\n",
        "  busca = memoria.search(palavra, top_n=6)\n",
        "  return busca"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKs0zciNBHL_"
      },
      "source": [
        "A palavra mais próxima armazenada na memória é ela mesma, portanto para encontrar as outras cinco palavras mais próximas foi recuperado as palavras de índice 1 até 6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBe9Eq_I-ozC",
        "outputId": "c18d3d91-caea-41ff-f8ce-695598b8db96",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initiliazing embeddings:  normal\n",
            "OK.\n",
            "[{'chunk': 'Biodiversidade', 'metadata': {}, 'distance': 0.0}, {'chunk': 'Sustentabilidade', 'metadata': {}, 'distance': 0.44970125}, {'chunk': 'Agroecologia', 'metadata': {}, 'distance': 0.492421}, {'chunk': 'Biotecnologia', 'metadata': {}, 'distance': 0.49487936}, {'chunk': 'Biodegradável', 'metadata': {}, 'distance': 0.5287854}, {'chunk': 'Bioma', 'metadata': {}, 'distance': 0.5513243}]\n",
            "\n",
            "\n",
            "Busca: Biodiversidade\n",
            "\n",
            "Palavra: Sustentabilidade\n",
            "Distância: 0.45\n",
            "\n",
            "Palavra: Agroecologia\n",
            "Distância: 0.49\n",
            "\n",
            "Palavra: Biotecnologia\n",
            "Distância: 0.49\n",
            "\n",
            "Palavra: Biodegradável\n",
            "Distância: 0.53\n",
            "\n",
            "Palavra: Bioma\n",
            "Distância: 0.55\n",
            "\n"
          ]
        }
      ],
      "source": [
        "m = Memory(chunking_strategy={\"mode\": \"sliding_window\", \"window_size\": 1, \"overlap\": 0})\n",
        "n = d[\"n_palavras\"]\n",
        "p = d[\"palavras\"]\n",
        "\n",
        "embedding(m, p, n)\n",
        "\n",
        "b = buscaVetorial(m, \"Biodiversidade\")\n",
        "print(b)\n",
        "print(f\"\\n\\nBusca: Biodiversidade\\n\")\n",
        "for i in range(1,6):\n",
        "  palavra = b[i]['chunk']\n",
        "  distancia = b[i]['distance']\n",
        "  print(f\"Palavra: {palavra}\\nDistância: {distancia:.2f}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQpdmDli23KZ"
      },
      "source": [
        "### Integração no grafo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "NuGHaDAJ25c-"
      },
      "outputs": [],
      "source": [
        "def integraGrafo(memoria, palavras, n_palavras):\n",
        "  grafo = GrafoNDR(n_palavras)\n",
        "  for i in range(n_palavras):\n",
        "    busca = buscaVetorial(memoria, palavras[i])\n",
        "    for j in range(1,6):\n",
        "      palavra = busca[j]['chunk']\n",
        "      distancia = busca[j]['distance']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsUUPVLa2JAs"
      },
      "source": [
        "## 2. Gravar dados"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "S8aX3NJd2JAs"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcUaBUFE2JAs"
      },
      "source": [
        "## 3. Inserir vértice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "rEUyOKTo2JAt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8PvH4ed2JAt"
      },
      "source": [
        "## 4. Inserir aresta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "6u2WV2hb2JAt"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMVke0Al2JAt"
      },
      "source": [
        "## 5. Remover vértice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "CqO6N1PP2JAu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvUaJjZS2JAu"
      },
      "source": [
        "## 6. Remover aresta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "hmD-cjiW2JAu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f8SKdKg2JAv"
      },
      "source": [
        "## 7. Mostrar conteúdo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "tlbI-jgd2JA1"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kS8x6Fjd2JA2"
      },
      "source": [
        "## 8. Mostrar grafo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "J6uG3Bl12JA2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hs6GqASM2JA2"
      },
      "source": [
        "## 9. Apresentar conexidade do grafo e o reduzido"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "sUTjaER52JA2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YNbDvEe2JA3"
      },
      "source": [
        "# Menu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "AdLQfMUkWbyX",
        "outputId": "5071352b-f96f-4275-bba9-2c6d194d94c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initiliazing embeddings:  normal\n",
            "OK.\n",
            "\n",
            "Menu:\n",
            "    1) Ler dados do arquivo em python\n",
            "    2) Gravar dados no arquivo grafo.txt\n",
            "    3) Inserir vértice\n",
            "    4) Inserir aresta\n",
            "    5) Remove vértice\n",
            "    6) Remove aresta\n",
            "    7) Mostrar conteúdo do arquivo\n",
            "    8) Mostrar grafo\n",
            "    9) Apresentar a conexidade do grafo e o reduzido\n",
            "    10) Encerrar a aplicação\n",
            "\n",
            "1\n",
            "Grafo lido com sucesso!\n",
            "\n",
            "Menu:\n",
            "    1) Ler dados do arquivo em python\n",
            "    2) Gravar dados no arquivo grafo.txt\n",
            "    3) Inserir vértice\n",
            "    4) Inserir aresta\n",
            "    5) Remove vértice\n",
            "    6) Remove aresta\n",
            "    7) Mostrar conteúdo do arquivo\n",
            "    8) Mostrar grafo\n",
            "    9) Apresentar a conexidade do grafo e o reduzido\n",
            "    10) Encerrar a aplicação\n",
            "\n",
            "10\n",
            "Encerrando programa...\n"
          ]
        }
      ],
      "source": [
        "memoria = Memory(chunking_strategy={\"mode\": \"sliding_window\", \"window_size\": 1, \"overlap\": 0})\n",
        "fim = False\n",
        "\n",
        "while(fim == False):\n",
        "    print(\n",
        "'''\n",
        "Menu:\n",
        "    1) Ler dados do arquivo em python\n",
        "    2) Gravar dados no arquivo grafo.txt\n",
        "    3) Inserir vértice\n",
        "    4) Inserir aresta\n",
        "    5) Remove vértice\n",
        "    6) Remove aresta\n",
        "    7) Mostrar conteúdo do arquivo\n",
        "    8) Mostrar grafo\n",
        "    9) Apresentar a conexidade do grafo e o reduzido\n",
        "    10) Encerrar a aplicação\n",
        "''')\n",
        "    choice = int(input())\n",
        "    if choice == 1: # Lê grafo\n",
        "        dados = leArquivoHTTP()\n",
        "        #dados = leArquivo(\"palavras.txt\")\n",
        "        embedding(memoria, dados[\"palavras\"], dados[\"n_palavras\"])\n",
        "        print(\"Grafo lido com sucesso!\")\n",
        "\n",
        "    elif choice == 2: # Grava dados no arquivo .txt\n",
        "        print(\"Dados salvos com sucesso!\")\n",
        "\n",
        "    elif choice == 3: # Insere vértice\n",
        "        print(\"Vértice inserido com sucesso!\")\n",
        "\n",
        "    elif choice == 4: # Insere aresta\n",
        "        print(\"Arestas inseridas com sucesso!\")\n",
        "\n",
        "    elif choice == 5: # Remove vértice\n",
        "        print(\"Vértice removido com sucesso!\")\n",
        "\n",
        "    elif choice == 6: # Remove varesta\n",
        "        print(\"Aresta removida com sucesso!\")\n",
        "\n",
        "    elif choice == 7: # Imprime arquivo\n",
        "        print(\"oi\")\n",
        "\n",
        "    elif choice == 8: # Exibe grafo\n",
        "        print(\"oi\")\n",
        "\n",
        "    elif choice == 9: # Apresenta a conexidade do grafo e grafo reduzido\n",
        "        print(\"oi\")\n",
        "\n",
        "    elif choice == 10: # Encerra\n",
        "        fim = True\n",
        "        print(\"Encerrando programa...\")\n",
        "\n",
        "    else:\n",
        "        print(\"Opção inválida.\")\n",
        "\n",
        "    time.sleep(4) # Volta para o menu após 4 segundos\n",
        "\n",
        "    if os.name == 'nt': # Limpa o terminal\n",
        "        os.system('cls') # Caso o OS seja Windows\n",
        "    else:\n",
        "        os.system('clear') # Caso o OS seja Linux ou MacOS"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "nL7lcvob_Vsv",
        "f4Fd36vBfY_m",
        "jUL4VcKo2JAl",
        "g2sI3d7Qz7ck",
        "Nt727Vzj0Kos",
        "XMuxrf07kEw_",
        "rsUUPVLa2JAs",
        "rcUaBUFE2JAs",
        "-8PvH4ed2JAt",
        "nMVke0Al2JAt",
        "cvUaJjZS2JAu",
        "4f8SKdKg2JAv",
        "kS8x6Fjd2JA2",
        "hs6GqASM2JA2"
      ],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}